{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv sdoh-venv\n",
    "!source ./sdoh-venv/bin/activate\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD_NAME = \"llm_multi_stage\"\n",
    "DATA_TYPE = \"crisis\" #[\"crisis\", \"categorical\"]\n",
    "DATA_CSV_PATH = f\"/data/sampled_balanced_{DATA_TYPE}_data.csv\"\n",
    "CRISIS_DEFINITION_JSON_PATH = f\"/data/{DATA_TYPE}_data_definition.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate LLM, gpt-35-turbo-16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import pandas as pd \n",
    "import json \n",
    "import logging \n",
    "logging.basicConfig(level=logging.INFO, filename=f\"{METHOD_NAME}.log\", filemode='w')\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = \"\",  \n",
    "  api_version = \"2024-05-01-preview\",\n",
    "  azure_endpoint = \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_CSV_PATH)\n",
    "\n",
    "with open(CRISIS_DEFINITION_JSON_PATH, 'r') as file:\n",
    "    crisis_definition_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRISIS_LIST = [key for key, _ in crisis_definition_dict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 - Relevance Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json.decoder import JSONDecodeError\n",
    "from retry import retry\n",
    "\n",
    "def parse_llm_stage1_response_content(response_content):\n",
    "    response_content = json.loads(response_content)\n",
    "    prediction = list(response_content['Relevant Descriptions'])\n",
    "    return prediction\n",
    "\n",
    "@retry((JSONDecodeError, ValueError), tries=3, delay=1)\n",
    "def llm_stage1_relevant_extraction(narrative, crisis_definition):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure-gpt-35-turbo-0125\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": f\"You are a helpful assistant. You will be given a report narrative, and the definition of a social determinants of health factor. Please read the given narrative, and find the sentences that are closely relevant to the factor of interest.\"},\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"This is the definition of the factor of interest: {crisis_definition}. This is the report narrative: {narrative}. Output the list of closely relevant sentences, or an empty list if there is no relevant sentence, in the format of a valid JSON payload with keys \\\"Relevant Descriptions\\\".\"}\n",
    "        ]    \n",
    "    )\n",
    "    response_content = response.choices[0].message.content\n",
    "    try:\n",
    "        prediction = parse_llm_stage1_response_content(response_content)\n",
    "    except Exception as e:\n",
    "        print(response_content)\n",
    "        prediction = []\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for CRISIS in CRISIS_LIST:\n",
    "    CRISIS_RESULT_DICT = []\n",
    "    CRISIS_DEFINITION = crisis_definition_dict[CRISIS]\n",
    "    CRISIS_TEST_DATA = df[df['Target_Class'].isin([CRISIS])].reset_index(drop=True)\n",
    "    for idx, row in tqdm(CRISIS_TEST_DATA.iterrows(), total=len(CRISIS_TEST_DATA)):\n",
    "        narrative = \"\"\n",
    "        narrative_cme, narrative_le = row['NarrativeCME'], row['NarrativeLE']\n",
    "        if not pd.isna(narrative_cme):\n",
    "            narrative += narrative_cme\n",
    "        if not pd.isna(narrative_le):\n",
    "            narrative += narrative_le \n",
    "        \n",
    "        if narrative != \"\":\n",
    "            relevant_sentences = llm_stage1_relevant_extraction(narrative, CRISIS_DEFINITION)\n",
    "            CRISIS_RESULT_DICT.append({'PersonID': row['PersonID'],\n",
    "                                       'Ground_Truth': row['Target_Class_Value'], \n",
    "                                       'Stage_1_Relevant_Sentences': relevant_sentences})\n",
    "    \n",
    "    with open(f'/{METHOD_NAME}_results/stage_1_relevant_sentences_{CRISIS}_rerun.json', 'w') as file:\n",
    "        json.dump(CRISIS_RESULT_DICT, file, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 - Relevance Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json.decoder import JSONDecodeError\n",
    "from retry import retry\n",
    "\n",
    "def parse_llm_stage2_response_content(response_content):\n",
    "    response_content = json.loads(response_content)\n",
    "    prediction = response_content['Relevant']\n",
    "    return prediction\n",
    "\n",
    "@retry((JSONDecodeError, ValueError), tries=3, delay=1)\n",
    "def llm_stage2_relevant_verification(sentence, crisis_definition):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure-gpt-35-turbo-0125\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": f\"You are a helpful assistant. You will be given a sentence, and the definition of a social determinants of health factor. Please read the given sentence, and answer if the sentence is describing the factor of interest.\"},\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"This is the definition of the factor of interest: {crisis_definition}. This is the sentence: {sentence}. Output True or False in the format of a valid JSON payload with keys \\\"Relevant\\\".\"}\n",
    "        ]    \n",
    "    )\n",
    "    response_content = response.choices[0].message.content\n",
    "    try:\n",
    "        prediction = parse_llm_stage2_response_content(response_content)\n",
    "    except Exception as e:\n",
    "        print(response_content)\n",
    "        prediction = False\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [09:39<00:00,  1.04it/s]\n",
      "100%|██████████| 600/600 [10:23<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "for CRISIS in CRISIS_LIST:\n",
    "    CRISIS_DEFINITION = crisis_definition_dict[CRISIS]\n",
    "    CRISIS_STAGE2_RESULT_DICT = []\n",
    "    with open(f'/{METHOD_NAME}_results/stage_1_relevant_sentences_{CRISIS}_rerun.json', 'r') as file:\n",
    "        CRISIS_RESULT_DICT = json.load(file)\n",
    "    \n",
    "    for idx in tqdm(range(len(CRISIS_RESULT_DICT))):\n",
    "        entry = CRISIS_RESULT_DICT[idx]\n",
    "        personid = entry['PersonID']\n",
    "        ground_truth = entry['Ground_Truth']\n",
    "        relevant_sentences = entry['Stage_1_Relevant_Sentences']\n",
    "        if relevant_sentences is None or len(relevant_sentences) == 0:\n",
    "            verified_relevant_sentences = []\n",
    "        else:\n",
    "            verified_relevant_sentences = []\n",
    "            for sentence in relevant_sentences:\n",
    "                prediction = llm_stage2_relevant_verification(sentence, CRISIS_DEFINITION)\n",
    "                if str(prediction).lower() == 'true':\n",
    "                    verified_relevant_sentences.append(sentence)\n",
    "                else:\n",
    "                    continue \n",
    "        \n",
    "        CRISIS_STAGE2_RESULT_DICT.append({'PersonID': personid,\n",
    "                                            'Ground_Truth': ground_truth, \n",
    "                                            'Stage_1_Relevant_Sentences': relevant_sentences,\n",
    "                                            'Stage_2_Relevant_Sentences_Verified': verified_relevant_sentences})\n",
    "    \n",
    "    with open(f'/{METHOD_NAME}_results/stage_2_relevant_sentences_{CRISIS}_rerun.json', 'w') as file:\n",
    "        json.dump(CRISIS_STAGE2_RESULT_DICT, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 - Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json.decoder import JSONDecodeError\n",
    "from retry import retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_llm_stage3_response_content(response_content):\n",
    "    response_content = json.loads(response_content)\n",
    "    prediction = response_content['Prediction']\n",
    "    return prediction\n",
    "\n",
    "@retry((JSONDecodeError, ValueError), tries=3, delay=1)\n",
    "def llm_stage3_decision_making(sentences, crisis_definition):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure-gpt-35-turbo-0125\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": f\"You are a helpful assistant. You will be given a list of sentences, and the definition of a social determinants of health factor. Please read the given sentences, and answer if the factor of interest contributed to the suicide incident.\"},\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"This is the definition of the factor of interest: {crisis_definition}. This are the sentences: {sentences}. Output True or False in the format of a valid JSON payload with key \\\"Prediction\\\".\"}\n",
    "        ]    \n",
    "    )\n",
    "    response_content = response.choices[0].message.content\n",
    "    try:\n",
    "        prediction = parse_llm_stage3_response_content(response_content)\n",
    "    except Exception as e:\n",
    "        print(response_content)\n",
    "        prediction = False\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [03:19<00:00,  3.01it/s]\n",
      "100%|██████████| 600/600 [03:10<00:00,  3.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for CRISIS in CRISIS_LIST:\n",
    "    CRISIS_DEFINITION = crisis_definition_dict[CRISIS]\n",
    "    CRISIS_STAGE3_RESULT_DICT = []\n",
    "    with open(f'/{METHOD_NAME}_results/stage_2_relevant_sentences_{CRISIS}_rerun.json', 'r') as file:\n",
    "        CRISIS_RESULT_DICT = json.load(file)\n",
    "    \n",
    "    for idx in tqdm(range(len(CRISIS_RESULT_DICT))):\n",
    "        entry = CRISIS_RESULT_DICT[idx]\n",
    "        personid = entry['PersonID']\n",
    "        ground_truth = entry['Ground_Truth']\n",
    "        relevant_sentences = entry['Stage_1_Relevant_Sentences']\n",
    "        verified_relevant_sentences = entry['Stage_2_Relevant_Sentences_Verified']\n",
    "        if verified_relevant_sentences is None or len(verified_relevant_sentences) == 0:\n",
    "            prediction_value = 0\n",
    "        else:\n",
    "            prediction = llm_stage3_decision_making(verified_relevant_sentences, CRISIS_DEFINITION)\n",
    "            if str(prediction).lower() == 'true':\n",
    "                prediction_value = 1\n",
    "            else:\n",
    "                prediction_value = 0\n",
    "        \n",
    "        CRISIS_STAGE3_RESULT_DICT.append({'PersonID': personid,\n",
    "                                            'Ground_Truth': ground_truth, \n",
    "                                            'Stage_1_Relevant_Sentences': relevant_sentences,\n",
    "                                            'Stage_2_Relevant_Sentences_Verified': verified_relevant_sentences,\n",
    "                                            'Stage_3_Decision_Making': prediction_value})\n",
    "    \n",
    "    with open(f'/{METHOD_NAME}_results/stage_3_prediction_{CRISIS}_rerun.json', 'w') as file:\n",
    "        json.dump(CRISIS_STAGE3_RESULT_DICT, file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CRISIS in CRISIS_LIST:\n",
    "    CRISIS_RESULT_DICT = []\n",
    "    with open(f'/{METHOD_NAME}_results/stage_3_prediction_{CRISIS}_rerun.json', 'r') as file:\n",
    "        stage3_results = json.load(file)\n",
    "\n",
    "    # CRISIS_TEST_DATA = df[df['Target_Class'].isin([CRISIS])].reset_index(drop=True)\n",
    "    # gt_dict = {}\n",
    "    # for idx, row in CRISIS_TEST_DATA.iterrows():\n",
    "    #     gt_dict[row['PersonID']] = row['Target_Class_Value']\n",
    "\n",
    "    for entry in stage3_results:\n",
    "        CRISIS_RESULT_DICT.append({'PersonID': entry['PersonID'],\n",
    "                                   'ground_truth': entry['Ground_Truth'], #gt_dict[entry['PersonID']],\n",
    "                                   'prediction_value': entry['Stage_3_Decision_Making']})\n",
    "    \n",
    "    with open(f'/{METHOD_NAME}_results/{CRISIS}_rerun.json', 'w') as file:\n",
    "        json.dump(CRISIS_RESULT_DICT, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for CRISIS in CRISIS_LIST:\n",
    "    CRISIS_TEST_DATA = df[df['Target_Class'].isin([CRISIS])].reset_index(drop=True)\n",
    "    gt_dict = {}\n",
    "    for idx, row in CRISIS_TEST_DATA.iterrows():\n",
    "        gt_dict[row['PersonID']] = row['Target_Class_Value']\n",
    "    \n",
    "    with open(f'/{METHOD_NAME}_results/stage_3_prediction_{CRISIS}.json', 'r') as file:\n",
    "        CRISIS_RESULT_DICT = json.load(file)\n",
    "    \n",
    "    gt_list, pred_list = [], []\n",
    "    for entry in CRISIS_RESULT_DICT:\n",
    "        personid = entry['PersonID']\n",
    "        gt_list.append(gt_dict[personid])\n",
    "        pred_list.append(entry['Stage_3_Decision_Making'])\n",
    "    \n",
    "    print(f'{CRISIS}')\n",
    "    print(classification_report(y_pred=pred_list, y_true=gt_list, digits=3))\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_test_data(data):\n",
    "    sampled_test_data = random.choices(data, k=len(data))\n",
    "    predictions, ground_truths = [], []\n",
    "    for entry in sampled_test_data:\n",
    "        predictions.append(entry['prediction_value'])\n",
    "        ground_truths.append(entry['ground_truth'])\n",
    "    return predictions, ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_data(crisis, data, n=1000):\n",
    "    bootstrap_dict = {}\n",
    "    for i in range(n):\n",
    "        predictions, ground_truths = sample_test_data(data)\n",
    "        bootstrap_dict[i] = {'prediction': predictions,\n",
    "                             'ground_truth': ground_truths}\n",
    "        \n",
    "    with open(f\"/{METHOD_NAME}_results/bootstrap_samples/bootstrap_{crisis}_rerun.json\", \"w\") as file:\n",
    "        json.dump(bootstrap_dict, file, indent=2)\n",
    "        \n",
    "    return bootstrap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_bootstrapped_f1(bootstrap_dict):\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "    for _, value in bootstrap_dict.items():\n",
    "        pred, gt = value['prediction'], value['ground_truth']\n",
    "        precision_score_value, recall_score_value, f1_score_value = precision_score(y_true=gt, y_pred=pred, average='macro'), recall_score(y_true=gt, y_pred=pred, average='macro'), f1_score(y_true=gt, y_pred=pred, average='macro')\n",
    "        precision_list.append(precision_score_value)\n",
    "        recall_list.append(recall_score_value)\n",
    "        f1_list.append(f1_score_value)\n",
    "    return precision_list, recall_list, f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "bootstrap_confidence_interval_result_file = f'/{METHOD_NAME}_results/bootstrap_confidence_interval_results_rerun.json'\n",
    "def write_bootstrap_results_to_file(crisis, score_name, mean, lower, higher):\n",
    "    if os.path.exists(bootstrap_confidence_interval_result_file):\n",
    "        with open(bootstrap_confidence_interval_result_file, 'r') as file:\n",
    "            confidence_interval_dict = json.load(file)\n",
    "    else:\n",
    "        confidence_interval_dict = {}\n",
    "    \n",
    "    if crisis in confidence_interval_dict:\n",
    "        crisis_dict = confidence_interval_dict[crisis]\n",
    "    else:\n",
    "        crisis_dict = {}\n",
    "    \n",
    "    if score_name in crisis_dict:\n",
    "        score_dict = crisis_dict[score_name]\n",
    "    else:\n",
    "        score_dict = {}\n",
    "    \n",
    "    score_dict['mean'] = mean \n",
    "    score_dict['ci_lower'] = lower\n",
    "    score_dict['ci_higher'] = higher \n",
    "\n",
    "    crisis_dict[score_name] = score_dict \n",
    "    confidence_interval_dict[crisis] = crisis_dict \n",
    "\n",
    "    with open(bootstrap_confidence_interval_result_file, 'w') as file:\n",
    "        json.dump(confidence_interval_dict, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "for CRISIS in CRISIS_LIST:\n",
    "    with open(f'/{METHOD_NAME}_results/{CRISIS}_rerun.json', 'r') as file:\n",
    "        CRISIS_RESULT_VALUE_DICT = json.load(file)\n",
    "    \n",
    "    bootstrap_dict = bootstrap_data(CRISIS, CRISIS_RESULT_VALUE_DICT)\n",
    "    precision_list, recall_list, f1_list = compute_bootstrapped_f1(bootstrap_dict)\n",
    "\n",
    "    with open(f\"/{METHOD_NAME}_results/bootstrap_samples/bootstrap_precision_list_{CRISIS}_rerun.json\", \"w\") as file:\n",
    "        json.dump(precision_list, file, indent=2)\n",
    "\n",
    "    with open(f\"/{METHOD_NAME}_results/bootstrap_samples/bootstrap_recall_list_{CRISIS}_rerun.json\", \"w\") as file:\n",
    "        json.dump(recall_list, file, indent=2)\n",
    "    \n",
    "    with open(f\"/{METHOD_NAME}_results/bootstrap_samples/bootstrap_f1_list_{CRISIS}_rerun.json\", \"w\") as file:\n",
    "        json.dump(f1_list, file, indent=2)\n",
    "\n",
    "    print(f\"---{CRISIS}---\")\n",
    "    data = (precision_list,)  # samples must be in a sequence\n",
    "    bootstrap_ci = bootstrap(data, np.mean, confidence_level=0.95, random_state=1, method='percentile')\n",
    "    #print(f\"({bootstrap_ci.confidence_interval.low:.4};{bootstrap_ci.confidence_interval.high:.4})\")\n",
    "    write_bootstrap_results_to_file(CRISIS, 'precision', pd.Series(precision_list).mean(), bootstrap_ci.confidence_interval.low, bootstrap_ci.confidence_interval.high)\n",
    "\n",
    "    data = (recall_list,)  # samples must be in a sequence\n",
    "    bootstrap_ci = bootstrap(data, np.mean, confidence_level=0.95, random_state=1, method='percentile')\n",
    "    #print(f\"({bootstrap_ci.confidence_interval.low:.4};{bootstrap_ci.confidence_interval.high:.4})\")\n",
    "    write_bootstrap_results_to_file(CRISIS, 'recall', pd.Series(precision_list).mean(), bootstrap_ci.confidence_interval.low, bootstrap_ci.confidence_interval.high)\n",
    "\n",
    "    data = (f1_list,)  # samples must be in a sequence\n",
    "    bootstrap_ci = bootstrap(data, np.mean, confidence_level=0.95, random_state=1, method='percentile')\n",
    "    print(f\"({bootstrap_ci.confidence_interval.low:.4};{bootstrap_ci.confidence_interval.high:.4})\")\n",
    "    write_bootstrap_results_to_file(CRISIS, 'f1', pd.Series(precision_list).mean(), bootstrap_ci.confidence_interval.low, bootstrap_ci.confidence_interval.high)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
